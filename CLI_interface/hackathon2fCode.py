# -*- coding: utf-8 -*-
"""Hackathon2_Groupe

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16FxjWHBqgGVG6A26hsoCy2qwXTJTS3tc

# **Sujet 2 : LLM finement ajust√© pour classification de sentiments + r√©ponses contextuelles**

**Objectif**


Cr√©er un mini-LLM qui peut :
* Classifier une ¬¥emotion (positif, n√©gatif, neutre),
* Fournir une r√©ponse bas√©e sur des documents (type RAG).


**√©tapes guid√©es**
1. Choisir le jeu de donn√©es : par exemple IMDB (film reviews). Revoir : encoding.pdf,
Supervis√© mesures perfs.pdf.
2. Chargement du mod√©le : utilisez distilbert ou roberta-small.
3. Fine-tuning LoRA (simple) :
‚Ä¢ Utilisez le package peft pour faire un fine-tuning l√©ger.
‚Ä¢ Vous pouvez figer les poids et ne fine-tuner qu‚Äôune petite partie.
4. Cr√©ation du syst√®me RAG :
* Embedding avec sentence-transformers.
* Similarit√© cosinus
* G√©n√©ration via prompt + contexte.
* Revoir : GenAI RAG 1.pdf, GenAI RAG 2.pdf.
5. Interface simple : interface CLI pour faire une bo√Æte de texte et afficher la r√©ponse, si si le temps le permet, essayer Streamlit.

# **ETAPE 1**

installation des librairies

importation des modules

importation & √©chantillonnage des donn√©es

## etape 1.1 installation des librairies
"""

# Installation des librairies
!pip install -q torch transformers datasets accelerate peft scikit-learn numpy pandas tqdm openpyxl

"""## etape 1.2 importation modules

cellule √† compl√©ter au fur et mesure
"""

# importation de librairies /modules dans le script
import os  # operations fichiers / syst√®me
import json  # fichiers JSON data charger/sauvegarder
import time  # mesure latence
from datetime import datetime  # timestamps
from collections import Counter  # compter le repetitions

# Data
import pandas as pd
import numpy as np

# Machine Learning
from sklearn.feature_extraction.text import TfidfVectorizer  #  TF-IDF
from sklearn.neighbors import NearestNeighbors  #  kNN search
from sklearn.metrics.pairwise import cosine_similarity  #  similarit√© cosinus
from sklearn.model_selection import train_test_split  # train/val split

# Hugging Face
from datasets import load_dataset  # charger IMDB dataset
from transformers import (
    AutoTokenizer,  #tokenization
    AutoModelForSequenceClassification,  #  classification
    AutoModel,  #  embedding
    AutoModelForCausalLM,  #  text generation model
    TrainingArguments,  #  training configuration
    Trainer,  #  training the model
    pipeline
)

# LoRA fine-tuning
from peft import (
    LoraConfig,  # configuration
    get_peft_model,  # appliquer LoRA
    TaskType  # specifier task type
)

# barres de progression
from tqdm import tqdm

# PyTorch
import torch
from torch.utils.data import Dataset  # For custom datasets

print("import successful")
print(f"PyTorch version: {torch.__version__}")
print(f"CUDA available: {torch.cuda.is_available()}")  # Check if GPU is available

"""## etape 1.3 : charger et pr√©parer le dataset IMDB

√©chantillonnage 10 000 r√©view : √©quilibrer positives/n√©gatives


"""

# chargement donn√©es depuis Hugging Face
# movie reviews : positive/negative labels
print("Loading IMDB dataset...")
from datasets import load_dataset, concatenate_datasets # Import concatenate_datasets
dataset = load_dataset("imdb")

# charger train et test splits
original_train_data = dataset["train"]  # Original training data
original_test_data = dataset["test"]  # Original test data

# --- echantillonnage equilibr√© / Training Data ---
TRAIN_NEG_SAMPLES = 5000
TRAIN_POS_SAMPLES = 5000

# isoler reviews negatives et positives
train_neg = original_train_data.filter(lambda x: x['label'] == 0)
train_pos = original_train_data.filter(lambda x: x['label'] == 1)

# Selection des reviews pour l'√©chantillon
train_neg = train_neg.select(range(min(TRAIN_NEG_SAMPLES, len(train_neg))))
train_pos = train_pos.select(range(min(TRAIN_POS_SAMPLES, len(train_pos))))

# Combiner and m√©langer les subsets
train_data = concatenate_datasets([train_neg, train_pos]) # librairie Datasets de Hugging Face
train_data = train_data.shuffle(seed=42) # librairie Datasets de Hugging Face

# --- √©chantillon √©quilibr√© / Test Data ---
TEST_NEG_SAMPLES = 1000
TEST_POS_SAMPLES = 1000

# isoler reviews negatives and positives des test data
test_neg = original_test_data.filter(lambda x: x['label'] == 0)
test_pos = original_test_data.filter(lambda x: x['label'] == 1)

# Selectionner l'√©chantillon
test_neg = test_neg.select(range(min(TEST_NEG_SAMPLES, len(test_neg))))
test_pos = test_pos.select(range(min(TEST_POS_SAMPLES, len(test_pos))))

# Combiner et m√©langer l'√©chantillon des test_data
test_data = concatenate_datasets([test_neg, test_pos]) # cf librairie datasets de Hugging Face
test_data = test_data.shuffle(seed=42)

print(f"total extrait Training : {len(train_data)} (Negative: {TRAIN_NEG_SAMPLES}, Positive: {TRAIN_POS_SAMPLES})")
print(f"total extrait Test : {len(test_data)} (Negative: {TEST_NEG_SAMPLES}, Positive: {TEST_POS_SAMPLES})")

# afficher des exemples de review+ label
for item in range(10):

  print("\nExemples de reviews:")
  print(f"Texte: {train_data[item]['text'][:200]}...")  # premiers 200 characters
  print(f"Label: {train_data[item]['label']} (0=negative, 1=positive)")

"""# **ETAPE 2** : Mod√®le de classification des reviews positives (label=1) / negatives (label=0) : DistilBERT (version all√©g√©e de BERT)

## √©tape 2.1: charger le tokenizer et le mod√®le pr√©-entrain√©
"""

# configuration du mod√®le
MODEL_NAME = "distilbert-base-uncased"
NUM_LABELS = 2  # classification binaire: positive (1) or negative (0)

# charger le tokenizer pour convertir le texte de reviews en tokens (nombres)
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

# charger le mod√®le de claissification pr√©-entrain√©
model = AutoModelForSequenceClassification.from_pretrained(
    MODEL_NAME,
    num_labels=NUM_LABELS
)

print("Modele et tokenizer charg√©s")
print(f"Param√®tres du mod√®le: {sum(p.numel() for p in model.parameters()):,}")  # Total parameters

"""## √©tape 2.2: Tokenizer le Dataset

tokenizer training et test data.

"""

# Fonction : tokenizer le dataset
def tokenize_function(examples):
    """
    Tokenize  examples.
    Args: examples: Dictionary with 'text' key containing review texts
    Returns: Tokenized inputs with 'input_ids', 'attention_mask', etc.
    """
    # - truncation=True: limiter la s√©quenca max_length
    # - padding=True: harmoniser la longueur des s√©quences
    # - max_length=512: Maximulongueur max d'une sequence length (=limite de BERT)
    return tokenizer(
        examples["text"],
        truncation=True,
        padding=True,
        max_length=512
    )

# tokenization des training data
print("Tokenizing training data...")
tokenized_train = train_data.map(
    tokenize_function,
    batched=True,  # Process in batches for speed
    remove_columns=["text"]  # Remove original text (we have tokens now)
)

print("Exemple de tokenization pour les donn√©es d'entra√Ænement:")
print(f"Original text: {train_data[0]['text'][:100]}...")
print(f"Input IDs: {tokenized_train[0]['input_ids'][:20]}...")
print(f"Attention Mask: {tokenized_train[0]['attention_mask'][:20]}...")

# Apply tokenization to test data
print("\nTokenizing test data...")
tokenized_test = test_data.map(
    tokenize_function,
    batched=True,
    remove_columns=["text"]
)

print("Exemple de tokenization pour les donn√©es de test:")
print(f"Original text: {test_data[0]['text'][:100]}...")
print(f"Input IDs: {tokenized_test[0]['input_ids'][:20]}...")
print(f"Attention Mask: {tokenized_test[0]['attention_mask'][:20]}...")

print("\n Tokenization termin√©e !")
print(f"Total training: {len(tokenized_train)}")
print(f"Total test: {len(tokenized_test)}")

"""## √©tape 2.3: Configure LoRA pour un Fine-Tuning plus rapide

r√©duction du nombre de param√®tres √† tuner

"""

# Configure LoRA parameters
lora_config = LoraConfig(
    task_type=TaskType.SEQ_CLS,  # Sequence classification task
    r=8,  # Rank: lower = fewer parameters (8 is a good balance)
    lora_alpha=16,  # Scaling factor for LoRA weights
    lora_dropout=0.1,  # Dropout rate for LoRA layers
    target_modules=["q_lin", "v_lin"]  # Which layers to apply LoRA to
    # "q_lin" and "v_lin" are query and value linear layers in attention
)

# Apply LoRA to the model
# This freezes the base model weights and adds trainable LoRA adapters
print("Applying LoRA to model...")
model = get_peft_model(model, lora_config)

# Print trainable parameters
# With LoRA, we only train a tiny fraction of parameters!
trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
total_params = sum(p.numel() for p in model.parameters())

print("‚úì LoRA applied successfully!")
print(f"Trainable parameters: {trainable_params:,} ({100 * trainable_params / total_params:.2f}% of total)")
print(f"Total parameters: {total_params:,}")

"""## √©tape 2.4: mise en place du process de training

We configure the training process: learning rate, batch size, number of epochs, etc.

"""

# Training arguments
# These control how the model is trained
training_args = TrainingArguments(
    output_dir="./results",  # Where to save model checkpoints
    num_train_epochs=2, #3,  # Number of training epochs (full passes through data)
    per_device_train_batch_size=8,  # Batch size per device (small for memory)
    per_device_eval_batch_size=8,  # Evaluation batch size
    learning_rate=2e-4,  # Learning rate (how fast model learns)
    weight_decay=0.01,  # Regularization to prevent overfitting
    logging_dir="./logs",  # Where to save logs
    logging_steps=100,  # Log every 100 steps
    eval_strategy="epoch",  # Evaluate at end of each epoch
    save_strategy="epoch",  # Save model at end of each epoch
    load_best_model_at_end=True,  # Load best model after training
    fp16=True,  # Use mixed precision (faster, less memory) if GPU available
    report_to="none"  # Don't report to external services
)

print("‚úì Training configuration set up!")

"""## √©tape 2.5: split entre train et validation


We split the training data into train and validation sets, then create a Trainer object to handle the training process.

"""

## NE PAS REFAIRE LE SPLIT : PARTIE A EFFACER/DEBUT
# # Split tokenized training data into train and validation sets
# # 80% for training, 20% for validation
# train_val_split = tokenized_train.train_test_split(test_size=0.2, seed=42)

# train_dataset = train_val_split["train"]  # Training set
# val_dataset = train_val_split["test"]  # Validation set
###PARTIE A EFFACER/FIN

# attribution des donn√©es pour le training et le test √† partir de l'√©chantillon
#  √©quilibr√© extrait du dataset IMDB au d√©but
train_dataset = tokenized_train
val_dataset = tokenized_test

print(f"Training samples: {len(train_dataset)}")
print(f"Validation samples: {len(val_dataset)}")

# Create a metric function for evaluation
# This computes accuracy during training
def compute_metrics(eval_pred):
    """
    Compute accuracy metric.

    Args:
        eval_pred: Tuple of (predictions, labels)

    Returns:
        Dictionary with accuracy score
    """
    predictions, labels = eval_pred
    # Get predicted class (0 or 1) from logits
    predictions = np.argmax(predictions, axis=1)
    # Calculate accuracy
    accuracy = (predictions == labels).mean()
    return {"accuracy": accuracy}

# Create the Trainer
# Trainer handles all the training loop, evaluation, and saving
trainer = Trainer(
    model=model,  # The model to train
    args=training_args,  # Training configuration
    train_dataset=train_dataset,  # Training data
    eval_dataset=val_dataset,  # Validation data
    compute_metrics=compute_metrics,  # How to compute metrics
    tokenizer=tokenizer  # Tokenizer for decoding
)

print("‚úì Trainer created and ready for training!")

"""## Step 3: Train the Model

Now we train the model! This may take some time depending on your hardware. With LoRA, it should be faster than full fine-tuning.

"""

# Train the model
# This will take several minutes depending on your hardware
print("Starting training...")
print("This may take a few minutes. Please wait...")

# Start training
trainer.train()

print("‚úì Training complete!")

# Evaluate on validation set
print("\nEvaluating on validation set...")
eval_results = trainer.evaluate()
print(f"Validation Accuracy: {eval_results['eval_accuracy']:.4f}")
print(f"Validation Loss: {eval_results['eval_loss']:.4f}")

"""## Step 4: Save the Fine-Tuned Model

We save the trained model so we can use it later without retraining.

"""

# Save the fine-tuned model
MODEL_SAVE_PATH = "./fine_tuned_sentiment_model"

print(f"Saving model to {MODEL_SAVE_PATH}...")
trainer.save_model(MODEL_SAVE_PATH)
tokenizer.save_pretrained(MODEL_SAVE_PATH)

print("‚úì Model saved successfully!")

# Also save the full model with LoRA adapters
# This allows us to load it later
model.save_pretrained(MODEL_SAVE_PATH)

"""## Step 5: Test the Sentiment Classifier

Let's test our fine-tuned model on some example reviews to see if it works correctly.

"""

# Create a sentiment analysis pipeline for easy inference
# This makes it easy to classify new text
sentiment_pipeline = pipeline(
    "sentiment-analysis",
    model=model,
    tokenizer=tokenizer,
    device=0 if torch.cuda.is_available() else -1  # Use GPU if available
)

# Test on some example reviews
test_reviews = [
    "This movie was absolutely fantastic! I loved every minute of it.",
    "Terrible movie, waste of time. Boring and poorly acted.",
    "It was okay, nothing special but not bad either.",
    "One of the best films I've ever seen. Highly recommended!",
    "Awful. Just awful. Don't watch this."
]

print("Testing sentiment classifier:\n")
for review in test_reviews:
    result = sentiment_pipeline(review)[0]
    label = "POSITIVE" if result["label"] == "LABEL_1" else "NEGATIVE"
    score = result["score"]
    print(f"Review: {review[:60]}...")
    print(f"  ‚Üí {label} (confidence: {score:.3f})\n")

"""## Step 6: Build the RAG System - Prepare Corpus and Embeddings

Now we build the RAG (Retrieval-Augmented Generation) system. This will:
1. Create embeddings of all reviews using BERT
2. Integration of normalization in the custom via "normalize_embeddings" attribute
3. Implement kNN search for retrieval

"""

# --- √âTAPE 1 : Remplacement du mod√®le d'embedding ---
# On utilise Sentence-BERT, sp√©cifiquement con√ßu pour la similarit√© s√©mantique.
# 'all-MiniLM-L6-v2' est un excellent standard, rapide et performant.
# 'all-mpnet-base-v2' est plus pr√©cis mais plus lent (√† tester si besoin).
from sentence_transformers import SentenceTransformer
import numpy as np # S'assurer que numpy est import√©

embedding_model_name = 'sentence-transformers/all-MiniLM-L6-v2'
print(f"Chargement du mod√®le d'embedding optimis√© : {embedding_model_name}")
embedding_model = SentenceTransformer(embedding_model_name)
# Plus besoin de tokenizer s√©par√©, SentenceTransformer g√®re tout.

# Passage sur GPU si disponible
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
embedding_model = embedding_model.to(device)
print(f"‚úì Mod√®le d'embedding charg√© sur {device}")

# --- Mise √† jour de la fonction get_embeddings ---
def get_embeddings(texts, batch_size=32):
    """Generate embeddings using SentenceTransformer (optimis√©)."""
    if isinstance(texts, str):
        texts = [texts]

    # .encode() g√®re le tokenizing, le padding, le pooling et la normalisation
    # On utilise show_progress_bar=True pour suivre l'avancement
    embeddings = embedding_model.encode(
        texts,
        batch_size=batch_size,
        convert_to_tensor=False, # On veut des numpy arrays pour FAISS
        show_progress_bar=True,
        device=device, # Utiliser le bon device
        normalize_embeddings=True # <--- OPTIMISATION : Normalisation L2 directe ici !
    )

    # Si le r√©sultat est un tenseur, on le convertit en numpy
    if isinstance(embeddings, torch.Tensor):
        embeddings = embeddings.cpu().numpy()

    # On s'assure que c'est du float32 (standard pour FAISS et calculs vectoriels)
    return embeddings.astype(np.float32)

print("‚úì Nouvelle fonction get_embeddings pr√™te avec Sentence-BERT + Normalisation.")

# Get all review texts from training data
print("\nExtracting review texts...")
# Assurez-vous que train_data est d√©fini avant ce bloc
corpus_texts = [item["text"] for item in train_data]
print(f"Corpus size: {len(corpus_texts)} reviews")

# Create embeddings for the corpus
print("\nCreating Sentence embeddings (this may take a few minutes)...")
# On utilise la nouvelle fonction get_embeddings
corpus_embeddings = get_embeddings(corpus_texts)

print(f"‚úì Embeddings created! Shape: {corpus_embeddings.shape}")

import re  # Import pour les expressions r√©guli√®res (nettoyage)

# Note: On a supprim√© l'√©tape TF-IDF qui √©tait redondante.
# Le syst√®me repose enti√®rement sur la recherche s√©mantique (Embeddings + kNN).

# Build kNN index using cosine similarity
# kNN finds the most similar documents to a query
print("\nBuilding kNN index...")
knn_model = NearestNeighbors(
    n_neighbors=10,  # Retrieve top 10 most similar documents
    metric="cosine"  # Use cosine similarity (measures angle between vectors)
)
knn_model.fit(corpus_embeddings)  # Fit on BERT embeddings
print("‚úì kNN index built!")

# Function to retrieve relevant documents
def retrieve_documents(query_text, top_k=5):
    """
    Retrieve top-k most similar documents to a query.
    Args:
        query_text: The query text
        top_k: Number of documents to retrieve
    Returns:
        List of dictionaries with text, similarity, and sentiment
    """
    # Get query embedding
    query_embedding = get_embeddings([query_text])

    # Find nearest neighbors
    distances, indices = knn_model.kneighbors(query_embedding, n_neighbors=top_k)

    # Get retrieved documents
    retrieved_docs = []
    for idx, dist in zip(indices[0], distances[0]):
        raw_text = corpus_texts[idx]

        # --- OPTIMISATION 1 : R√©cup√©ration du Sentiment ---
        # On r√©cup√®re le label d'origine (0 ou 1) via l'index
        # (Suppose que train_data est toujours disponible)
        label_id = train_data[int(idx)]['label']
        sentiment_str = "POSITIVE" if label_id == 1 else "NEGATIVE"

        # --- NETTOYAGE DU TEXTE ---
        clean_text = re.sub(r'<[^>]+>', ' ', raw_text)
        clean_text = re.sub(r'\s+', ' ', clean_text).strip()

        retrieved_docs.append({
            "text": clean_text,
            "similarity": 1 - dist,
            "sentiment": sentiment_str # Ajout de la m√©tadonn√©e
        })

    return retrieved_docs

# Test retrieval
print("\nTesting retrieval system (Optimized)...")
test_query = "I love action movies with great special effects"
retrieved = retrieve_documents(test_query, top_k=3)
print(f"\nQuery: {test_query}")
print("\nRetrieved documents:")
for i, doc in enumerate(retrieved, 1):
    print(f"{i}. [{doc['sentiment']}] Similarity: {doc['similarity']:.3f}")
    print(f"   Text: {doc['text'][:100]}...")

# Test de r√©cup√©ration (Retrieval) avec le mod√®le kNN
query_test = "The special effects were amazing and the plot was thrilling"

print(f"üîé Requ√™te : '{query_test}'")
print("-" * 50)

# Appel de la fonction de r√©cup√©ration qui utilise l'index kNN
# Elle convertit la requ√™te en vecteur et cherche les voisins les plus proches
retrieved_docs = retrieve_documents(query_test, top_k = 10)

# Affichage des r√©sultats
for i, doc in enumerate(retrieved_docs, 1):
    print(f"\nüìÑ R√©sultat #{i}")
    print(f"   Score de similarit√© (Cosinus) : {doc['similarity']:.4f}")
    print(f"   Extrait du texte : \"{doc['text'][:250]}...\"")

"""## Step 6.5: Save Corpus Data for CLI Use

We save the corpus texts and embeddings so they can be loaded by the CLI interface script.

"""

# Save corpus texts and embeddings for CLI use
# This allows the CLI script to load the data without recomputing embeddings
print("Saving corpus data for CLI use...")

# Save corpus texts as numpy array (for easy loading)
np.save("corpus_texts.npy", np.array(corpus_texts, dtype=object))

# Save embeddings
np.save("corpus_embeddings.npy", corpus_embeddings)

print("‚úì Corpus data saved!")
print("  - corpus_texts.npy")
print("  - corpus_embeddings.npy")
print("\nThese files can now be loaded by the CLI interface script.")

"""## Step 7 Custom du mod√®le de g√©n√©ration de texte √† partir du mod√®le entrain√©"""

from transformers import AutoModelForSeq2SeqLM, AutoTokenizer
import torch

# Load Flan-T5 for text generation
# Flan-T5 is an instruction-tuned model excellent for RAG tasks.
print("Loading Flan-T5 for text generation...")
generator_model_name = "google/flan-t5-base"
generator_tokenizer = AutoTokenizer.from_pretrained(generator_model_name)
generator_model = AutoModelForSeq2SeqLM.from_pretrained(generator_model_name)

# Move to device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
generator_model = generator_model.to(device)
generator_model.eval()  # Evaluation mode

print(f"‚úì Generator model loaded on {device}")

# Function to generate response using retrieved context
def generate_response(
    query,
    retrieved_docs,
    max_new_tokens=150,
    num_context_docs=2,
    max_doc_length=600
):
    # 1. Prepare Context WITHOUT Labels
    context_parts = []
    for doc in retrieved_docs[:num_context_docs]:
        # Take up to max_doc_length characters from each document
        context_parts.append(f"Review: {doc['text'][:max_doc_length]}")

    context = "\n\n".join(context_parts)

    # 2. Construct Optimized Prompt
    # On donne un r√¥le clair √† l'IA et des instructions pr√©cises
    if not context:
        # Special instruction if no context is available
        prompt = f"""You are an expert movie assistant. You were asked: '{query}'. However, no relevant reviews were found to answer this question. Please state that you cannot provide an answer based on available information.

Answer:"""
    else:
        prompt = f"""You are an expert movie assistant. Use the reviews below to answer the user's question.
If the reviews are negative, explain why. If positive, highlight the good points.
If the provided reviews do not contain enough information to fully answer the question, state that you cannot answer completely based on the given context.

Context Reviews:
{context}

User Question: {query}

Answer:"""

    # Tokenize prompt
    inputs = generator_tokenizer(prompt, return_tensors="pt", truncation=True, max_length=512).to(device)

    # Generate response
    with torch.no_grad():
        outputs = generator_model.generate(
            input_ids=inputs["input_ids"],
            attention_mask=inputs["attention_mask"],
            max_new_tokens=max_new_tokens,
            min_length=30,
            num_return_sequences=1,
            temperature=0.7,
            do_sample=True,
            no_repeat_ngram_size=2
        )

    # Decode generated text
    generated_text = generator_tokenizer.decode(outputs[0], skip_special_tokens=True)

    return generated_text.strip()

"""## Step 7.5 Test Modele G√©n√©ration de code"""

# Test generation with IMPROVED settings
print("\nTesting text generation with Flan-T5 (IMPROVED)...\n")
test_query = "What do people think about the special effects in avatar ?"

# Retrieval (now includes sentiment)
retrieved = retrieve_documents(test_query, top_k=5)

# Generate response (uses optimized prompt with sentiment context)
response = generate_response(test_query, retrieved)

print(f"Query: {test_query}")
print(f"\nGenerated Response (Length: {len(response.split())} words):")
print("-" * 50)
print(response)
print("-" * 50)

"""## Step 8 Sauvegarde du mod√®le"""

# Sauvegarde du mod√®le d'embeddings
# Note: SentenceTransformer g√®re son propre tokenizer interne et utilise .save()
MODEL_EMBEDDING_PATH = "./embedding_model"

print(f"Saving embedding model to {MODEL_EMBEDDING_PATH}...")
embedding_model.save(MODEL_EMBEDDING_PATH)
print("‚úì Embedding model saved successfully!")

# Sauvegarde du mod√®le de g√©n√©ration et son tokenizer
MODEL_GENERATOR_PATH = "./generator_model"

print(f"Saving generator model to {MODEL_GENERATOR_PATH}...")
generator_model.save_pretrained(MODEL_GENERATOR_PATH)
generator_tokenizer.save_pretrained(MODEL_GENERATOR_PATH)
print("‚úì Generator model saved successfully!")

# import shutil
# import os

# # Define paths for the fine-tuned sentiment model and its tokenizer
# MODEL_SAVE_PATH = "./fine_tuned_sentiment_model"

# # 1. Compresser le dossier du mod√®le de sentiment affin√©
# model_dir = MODEL_SAVE_PATH
# zip_file_name_sentiment = "fine_tuned_sentiment_model.zip"

# # Cr√©er l'archive zip
# print(f"Compression du dossier '{model_dir}' en '{zip_file_name_sentiment}'...")
# shutil.make_archive(model_dir, 'zip', model_dir)
# print("‚úì Dossier du mod√®le de sentiment compress√© avec succ√®s !")

# # 2. T√©l√©charger le fichier zip du mod√®le de sentiment
# from google.colab import files

# print(f"T√©l√©chargement de '{zip_file_name_sentiment}'...")
# files.download(zip_file_name_sentiment)
# print("‚úì T√©l√©chargement du mod√®le de sentiment lanc√© !")


# # Compresser le dossier du mod√®le d'embeddings
# embedding_model_dir = './embedding_model'
# embedding_zip_file_name = "embedding_model.zip"
# print(f"Compression du dossier '{embedding_model_dir}' en '{embedding_zip_file_name}'...")
# shutil.make_archive(embedding_model_dir, 'zip', embedding_model_dir)
# print("‚úì Dossier d'embeddings compress√© avec succ√®s !")

# # Compresser le dossier du mod√®le de g√©n√©ration
# generator_model_dir = './generator_model'
# generator_zip_file_name = "generator_model.zip"
# print(f"Compression du dossier '{generator_model_dir}' en '{generator_zip_file_name}'...")
# shutil.make_archive(generator_model_dir, 'zip', generator_model_dir)
# print("‚úì Dossier de g√©n√©ration compress√© avec succ√®s !")

# # T√©l√©charger les fichiers zip des mod√®les d'embedding et de g√©n√©ration
# print(f"T√©l√©chargement de '{embedding_zip_file_name}'...")
# files.download(embedding_zip_file_name)

# print(f"T√©l√©chargement de '{generator_zip_file_name}'...")
# files.download(generator_zip_file_name)

# # T√©l√©charger les fichiers corpus_embeddings.npy et corpus_texts.npy
# print("T√©l√©chargement de 'corpus_embeddings.npy'...")
# files.download('corpus_embeddings.npy')

# print("T√©l√©chargement de 'corpus_texts.npy'...")
# files.download('corpus_texts.npy')

# print("‚úì T√©l√©chargements lanc√©s !")

"""## Step 9: Create Evaluation Functions

We create functions to evaluate the system on multiple metrics: accuracy, response length, repetitions, keyword presence, and latency.

"""

# Evaluation functions

def count_repetitions(text, n=3):
    """
    Count repeated n-grams (phrases) in text.
    Higher repetition = lower quality.

    Args:
        text: Text to analyze
        n: Length of n-grams to check

    Returns:
        Number of repeated n-grams
    """
    words = text.lower().split()
    ngrams = [tuple(words[i:i+n]) for i in range(len(words)-n+1)]
    ngram_counts = Counter(ngrams)
    # Count n-grams that appear more than once
    repetitions = sum(1 for count in ngram_counts.values() if count > 1)
    return repetitions

def check_keyword_presence(text, keywords):
    """
    Check if important keywords from query appear in response.

    Args:
        text: Response text
        keywords: List of keywords to check

    Returns:
        Number of keywords found
    """
    text_lower = text.lower()
    found = sum(1 for keyword in keywords if keyword.lower() in text_lower)
    return found

def evaluate_response(query, response, true_label=None, start_time=None):
    """
    Evaluate a single response on multiple metrics.

    Args:
        query: Original query
        response: Generated response
        true_label: True sentiment label (if available)
        start_time: Start time for latency calculation

    Returns:
        Dictionary with evaluation metrics
    """
    metrics = {}

    # Response length
    metrics["response_length"] = len(response.split())

    # Repetitions
    metrics["repetitions"] = count_repetitions(response)

    # Keyword presence (extract important words from query)
    query_words = [w for w in query.lower().split() if len(w) > 3]  # Words longer than 3 chars
    metrics["keywords_found"] = check_keyword_presence(response, query_words)

    # Latency (time taken to generate)
    if start_time:
        metrics["latency"] = time.time() - start_time

    # Classification accuracy (if true label provided)
    if true_label is not None:
        result = sentiment_pipeline(query)[0]
        predicted_label = 1 if result["label"] == "LABEL_1" else 0
        metrics["classification_correct"] = (predicted_label == true_label)
        metrics["classification_confidence"] = result["score"]

    return metrics

print("‚úì Evaluation functions created!")

"""## Step 10: Run End-to-End Evaluation

We test the complete system on 50 queries and collect all evaluation metrics.

"""

# Create test queries for evaluation
# Mix of different types of queries
test_queries = [
    "What did people think about the plot?",
    "How was the acting?",
    "Was the movie entertaining?",
    "Did people like the special effects?",
    "What about the cinematography?",
    "How was the dialogue?",
    "Was the movie well-directed?",
    "Did people enjoy the soundtrack?",
    "How was the pacing?",
    "What did people think about the ending?",
] * 5 # On fait un test rapide sur 10 requ√™tes (sans multiplier par 5)

# Run evaluation
print(f"Running evaluation on {len(test_queries)} queries (Quick Test)...")
print("This may take a minute...\n")

# --- FIX: Ensure sentiment_pipeline is defined ---
try:
    sentiment_pipeline
except NameError:
    print("sentiment_pipeline not found. Initializing it now...")
    sentiment_pipeline = pipeline(
        "sentiment-analysis",
        model=model,
        tokenizer=tokenizer,
        device=0 if torch.cuda.is_available() else -1
    )
# -----------------------------------------------

results = []

for i, query in enumerate(tqdm(test_queries, desc="Evaluating")):
    start_time = time.time()

    # Get sentiment classification
    sentiment_result = sentiment_pipeline(query)[0]
    predicted_sentiment = "POSITIVE" if sentiment_result["label"] == "LABEL_1" else "NEGATIVE"

    # Retrieve relevant documents
    retrieved = retrieve_documents(query, top_k=5)

    # Generate response
    response = generate_response(query, retrieved)

    # Evaluate
    metrics = evaluate_response(query, response, start_time=start_time)

    # Store results
    results.append({
        "query": query,
        "sentiment": predicted_sentiment,
        "sentiment_confidence": sentiment_result["score"],
        "response": response,
        **metrics  # Add all evaluation metrics
    })

# Convert to DataFrame for easy analysis
results_df = pd.DataFrame(results)

# Calculate summary statistics
print("\n" + "="*60)
print("EVALUATION RESULTS (Optimized System)")
print("="*60)
print(f"\nTotal queries: {len(results_df)}")
print(f"Average response length: {results_df['response_length'].mean():.1f} words")
print(f"Average repetitions: {results_df['repetitions'].mean():.1f}")
print(f"Average keywords found: {results_df['keywords_found'].mean():.1f}")
print(f"Average latency: {results_df['latency'].mean():.2f} seconds")
print(f"\nSentiment distribution:")
print(results_df['sentiment'].value_counts())

# Save results to CSV
results_df.to_csv("evaluation_results_optimized.csv", index=False)
print("\n‚úì Results saved to 'evaluation_results_optimized.csv'")

"""## Step 11: Robustness Tests

We test the system with edge cases: vague prompts, contradictory prompts, and unrelated prompts.

"""

# Robustness tests
print("="*60)
print("ROBUSTNESS TESTS")
print("="*60)

# Test 1: Vague prompt
print("\n1. VAGUE PROMPT TEST")
print("-" * 60)
vague_query = "What about it?"
print(f"Query: '{vague_query}'")
retrieved = retrieve_documents(vague_query, top_k=3)
response = generate_response(vague_query, retrieved)
print(f"Response: {response[:200]}...")
metrics = evaluate_response(vague_query, response)
print(f"Metrics: Length={metrics['response_length']}, Repetitions={metrics['repetitions']}")

# Test 2: Contradictory prompt
print("\n2. CONTRADICTORY PROMPT TEST")
print("-" * 60)
contradictory_query = "This movie is both amazing and terrible at the same time"
print(f"Query: '{contradictory_query}'")
sentiment_result = sentiment_pipeline(contradictory_query)[0]
print(f"Sentiment: {sentiment_result['label']} (confidence: {sentiment_result['score']:.3f})")
retrieved = retrieve_documents(contradictory_query, top_k=3)
response = generate_response(contradictory_query, retrieved)
print(f"Response: {response[:200]}...")

# Test 3: Unrelated prompt
print("\n3. UNRELATED PROMPT TEST")
print("-" * 60)
unrelated_query = "What is the weather like today?"
print(f"Query: '{unrelated_query}'")
retrieved = retrieve_documents(unrelated_query, top_k=3)
print(f"Retrieved documents similarity scores:")
for i, doc in enumerate(retrieved, 1):
    print(f"  {i}. {doc['similarity']:.3f}")
response = generate_response(unrelated_query, retrieved)
print(f"Response: {response[:200]}...")

print("\n" + "="*60)
print("Robustness tests complete!")
print("="*60)

"""## Step 12: Summary and Reflection

### Model Choices

**DistilBERT for Classification:**
- Smaller and faster than BERT
- Good performance for binary classification
- LoRA fine-tuning makes it efficient

**DistilGPT-2 for Generation:**
- Lightweight text generation model
- Fast inference
- Works well with retrieved context

### TF-IDF + kNN Pipeline

**TF-IDF (Term Frequency-Inverse Document Frequency):**
- Converts text to numerical vectors
- Highlights important words
- Fast and memory-efficient

**kNN (k-Nearest Neighbors):**
- Finds most similar documents using cosine similarity
- Works with BERT embeddings
- Simple and effective for retrieval

### CPU Limitations

- Training and inference can be slow on CPU
- GPU significantly speeds up:
  - Model training
  - Embedding creation
  - Text generation
- Google Colab provides free GPU access

### CLI Simplicity

- Interactive loop design
- Easy to use for beginners
- Integrates all components
- Collects feedback automatically

### Robustness Observations

1. **Vague prompts**: System may struggle with unclear queries
2. **Contradictory prompts**: Sentiment classifier picks one direction
3. **Unrelated prompts**: Retrieval may find loosely related documents

### Future Improvements

- Larger training dataset
- Better prompt engineering
- Fine-tune generator on movie reviews
- Use more sophisticated retrieval (e.g., dense passage retrieval)
- Implement better repetition detection and prevention

"""